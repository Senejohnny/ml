{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from metadata import data_type\n",
    "from src.pipe_store import (\n",
    "    data_loader,\n",
    "    date_parser,\n",
    "    clean_string_strip,\n",
    "    set_data_types,\n",
    "    sort_values_per_client,\n",
    "    datetime2int,\n",
    "    one_hot_encoder,\n",
    "    label_encoder,\n",
    "    summerize_client_behaviour,\n",
    ")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/Users/Danial/Downloads/assesment_file2_churn.csv'\n",
    "df = (\n",
    "    data_loader(data_path, parse_dates=['MONTH_PERIOD'], date_parser=date_parser)\n",
    "    .pipe(set_data_types, data_type) # optimize space & df size\n",
    "    .pipe(clean_string_strip, 'AGE_CLASS', 'HOMEBANK_COLOUR', 'LOYALITY')\n",
    "    .pipe(sort_values_per_client, 'MONTH_PERIOD')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Formulation\n",
    "\n",
    "Lets see how much information exist in the data set\n",
    "\n",
    "\n",
    "In this assessment case a customer is considered to have churned in a given month if either CHURNED_IND or COMMERCIALLY_CHURNED are set to value 1.\n",
    "\n",
    "### Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.feature_selection import total_variable_variances, variable_variances_per_client\n",
    "var_matrix = variable_variances_per_client(df)\n",
    "var_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_matrix.mean().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_variable_variances(df, top=6, include_label=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'Record_Count' & 'TARGET' should be discarded from the data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some remarks over the data\n",
    "\n",
    "* Why to summerize the data:\n",
    "    * The dynamic of client behaviours is rather slow (as to the variances). Thus we can account for the final value of each variable and the most recent changes (past 6 months) in the variable. This is to capture the latest state change in the variables close to the potential churn.\n",
    "\n",
    "    * This corrects for potential inconsistency in subscription start time is important to be taken \n",
    "    \n",
    "* To prevent potential bias in the dataset we have removed the client that either rejoin or rechurn \n",
    "\n",
    "* Column 'CHURNED_IND' is summerized to churn_event and the time at which churn occurs to churn_time.\n",
    "\n",
    "* If client is not churned during the 24 months churn_time is assumed to be an arbitrary date, e.g. 2013-01-01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_col = 'CHURNED_IND'\n",
    "horizon = 6 # Time horizon (Even number) to find dominant recent past states per column\n",
    "df_dropped = df.drop(['TARGET', 'Record_Count'], axis=1)\n",
    "df_sum = summerize_client_behaviour(df_dropped, churn_col=churn_col, horizon=horizon)\n",
    "df_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sum.churn_event.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sum.churn_event.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing Values\n",
    "Variables with missing values are: ACCOUNTMODEL, AGE_CLASS, HOMEBANK_COLOUR, LOYALITY\n",
    "\n",
    "### Type of missingness:\n",
    "\n",
    "In practice, domain experts or data managers can better argue on the type of the missingness. \n",
    "\n",
    "Missing Completely at Random (MCAR)-> random sampling from variable distribution\n",
    "\n",
    "Missing at Random (MAR)-> random sampling from variable distribution or predict missing class via Logistic Regression\n",
    "\n",
    "Missing not at Random (MNAR): Should not be imputed\n",
    "\n",
    "### Adopted Strategy\n",
    "\n",
    "Consider all the missingess as MNAR and proceed with feature selection. Upon importance of the variable with missigness or the missing categories we can come back and adopt another strategy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.eda import top_columns_with_missingness\n",
    "top_columns_with_missingness(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_columns_with_missingness(df_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sum = df_sum.fillna('Unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_sum.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AGE_CLASS columns has more missing value than reported due to \"Leeftijd_onbekend\" label which."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection\n",
    "\n",
    "From previous section we know to drop: 'Record_Count', 'TARGET'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = [\n",
    "    'CREDIT_CLASS', 'DEBIT_CLASS', 'INVESTED_CAPITAL_CLASS', 'SAVINGS_CAPITAL_CLASS', 'MIN_FEED_CLASS', 'REVENUES_CLASS',\n",
    "     'PAYMENT_ACTIVITIES_CODE', 'CLIENTGROUP', 'ACCOUNTMODEL', 'AGE_CLASS', 'HOMEBANK_COLOUR', 'LOYALITY'\n",
    "]\n",
    "df_corr = df_sum.copy(deep=True) \n",
    "df_corr = (\n",
    "    df_corr\n",
    "    .pipe(one_hot_encoder, *cat_cols, dtype='int8')\n",
    "    # .pipe(label_encoder, 'CLIENTGROUP', 'ACCOUNTMODEL', 'AGE_CLASS', 'HOMEBANK_COLOUR', 'LOYALITY')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2, f_classif\n",
    "col_drop = ['churn_event', 'churn_time', 'id', 'CROSS_SELL_SCORE']\n",
    "X, y = df_corr.drop(col_drop, axis=1), df_corr['churn_event'].astype('float16')\n",
    "k_best = SelectKBest(chi2, k=10).fit(X, y)\n",
    "selected_cols = list(k_best.get_feature_names_out())\n",
    "selected_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.feature_selection import plot_corr_cat\n",
    "\n",
    "corrs = plot_corr_cat(X[selected_cols], show_figure=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.eda import plot_graph\n",
    "\n",
    "plot_graph(corrs[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multicolinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.feature_selection import plot_corr_cat\n",
    "# plot_corr_cat(X[['CREDIT_CLASS', 'CREDIT_CLASS', 'AGE_CLASS']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# from seaborn import  heatmap\n",
    "# corr_mat = df_corr.corr()\n",
    "# plt.figure(figsize=(12, 8))\n",
    "# heatmap(corr_mat, cmap='RdYlGn', annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time-to-Event Featuer Selection\n",
    "\n",
    "For these kind of analysis we need to make the churn_time column into a float values showing months "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = {'churn_time': 'float16', 'churn_event': 'float16'}\n",
    "df_red = (\n",
    "    df_sum\n",
    "    .pipe(datetime2int, 'churn_time')\n",
    "    .pipe(set_data_types, dtype)\n",
    "    # .reset_index(drop=True)\n",
    "    .fillna('unknown')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seasonality [pattern in churn time]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_red[df_red.churn_event == 1]['churn_time'].value_counts().sort_index().plot.bar()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Univariate Time-to-Event feature analysis [Kaplan-Meier]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from src.eda import plot_km_churn_risk\n",
    "from lifelines import KaplanMeierFitter\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14, 4))\n",
    "# ('INSURANCE_LIFE_IND', 'INSURANCE_LIFE_IND'), ('MORTGAGE_IND', 'PACKAGE_IND'), ('INVESTMENTS_IND', 'LENDING_IND')\n",
    "for i, j in [('SAVING_IND', 'SAVING_IND_CHANGED')]:  # ('PAYMENT_IND_CHANGED', 'SAVING_IND_CHANGED', CROSS_SELL_SCORE_CHANGED\n",
    "    plot_km_churn_risk(df_red, i, ax=ax[0], estimator=KaplanMeierFitter, at_risk=True)\n",
    "    plot_km_churn_risk(df_red, j, ax=ax[1], estimator=KaplanMeierFitter, at_risk=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2, figsize=(14, 4))\n",
    "# plot_km_churn_risk(df_red, 'HOMEBANK_COLOUR', ax=ax[0], estimator=KaplanMeierFitter, at_risk=True)\n",
    "# plot_km_churn_risk(df_red, 'LOYALITY', ax=ax[1], estimator=KaplanMeierFitter, at_risk=True)\n",
    "plot_km_churn_risk(df_red, 'CROSS_SELL_SCORE', ax=ax[0], estimator=KaplanMeierFitter, at_risk=True)\n",
    "plot_km_churn_risk(df_red, 'CROSS_SELL_SCORE_CHANGED', ax=ax[1], estimator=KaplanMeierFitter, at_risk=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2, figsize=(14, 4))\n",
    "# 'CREDIT_CLASS', 'DEBIT_CLASS', 'INVESTED_CAPITAL_CLASS', 'SAVINGS_CAPITAL_CLASS', \n",
    "# 'MIN_FEED_CLASS', 'REVENUES_CLASS', 'PAYMENT_ACTIVITIES_CODE', 'CROSS_SELL_SCORE', 'CLIENTGROUP', 'ACCOUNTMODEL',\n",
    "# 'AGE_CLASS', 'HOMEBANK_COLOUR', 'LOYALITY']\n",
    "plot_km_churn_risk(df_red, 'LOYALITY', ax=ax[0], estimator=KaplanMeierFitter, at_risk=True)\n",
    "plot_km_churn_risk(df_red, 'LOYALITY_CHANGED', ax=ax[1], estimator=KaplanMeierFitter, at_risk=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2, figsize=(14, 4))\n",
    "plot_km_churn_risk(df_red, 'CREDIT_CLASS', ax=ax[0], estimator=KaplanMeierFitter, at_risk=True)\n",
    "plot_km_churn_risk(df_red, 'CREDIT_CLASS_CHANGED', ax=ax[1], estimator=KaplanMeierFitter, at_risk=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2, figsize=(14, 4)) \n",
    "plot_km_churn_risk(df_red, 'INVESTED_CAPITAL_CLASS', ax=ax[0], estimator=KaplanMeierFitter, at_risk=True)\n",
    "plot_km_churn_risk(df_red, 'SAVINGS_CAPITAL_CLASS', ax=ax[1], estimator=KaplanMeierFitter, at_risk=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2, figsize=(14, 4)) \n",
    "plot_km_churn_risk(df_red, 'MIN_FEED_CLASS', ax=ax[0], estimator=KaplanMeierFitter, at_risk=True)\n",
    "plot_km_churn_risk(df_red, 'REVENUES_CLASS', ax=ax[1], estimator=KaplanMeierFitter, at_risk=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'CLIENTGROUP', 'ACCOUNTMODEL',\n",
    "fig, ax = plt.subplots(1,2, figsize=(14, 4)) \n",
    "plot_km_churn_risk(df_red, 'PAYMENT_ACTIVITIES_CODE', ax=ax[0], estimator=KaplanMeierFitter, at_risk=True)\n",
    "plot_km_churn_risk(df_red, 'CROSS_SELL_SCORE', ax=ax[1], estimator=KaplanMeierFitter, at_risk=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 5)) \n",
    "plot_km_churn_risk(df_red, 'CLIENTGROUP', ax=ax, estimator=KaplanMeierFitter, at_risk=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0101: 13%, 0307:23 %,  0105: 42%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_red.AGE_CLASS.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cox Partial Hazard Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = [\n",
    "    'CREDIT_CLASS', 'DEBIT_CLASS', 'INVESTED_CAPITAL_CLASS', 'SAVINGS_CAPITAL_CLASS', \n",
    "    'MIN_FEED_CLASS', 'REVENUES_CLASS', 'PAYMENT_ACTIVITIES_CODE', \n",
    "    'CLIENTGROUP', 'ACCOUNTMODEL', 'AGE_CLASS', 'HOMEBANK_COLOUR', 'LOYALITY', ]     # 'CROSS_SELL_SCORE', \n",
    "    \n",
    "df_cox = (\n",
    "    df_red\n",
    "    .pipe(one_hot_encoder, *cat_cols)\n",
    "    [[\n",
    "        'event', 'churn_time', \n",
    "        'PAYMENT_IND',\n",
    "        'SAVING_IND',\n",
    "        # 'CREDIT_CLASS_0', \n",
    "        'CREDIT_CLASS_1', #\n",
    "        'CREDIT_CLASS_2',\n",
    "        # 'DEBIT_CLASS_0', #\n",
    "        # 'DEBIT_CLASS_1', #\n",
    "        # 'DEBIT_CLASS_2', #\n",
    "        'SAVINGS_CAPITAL_CLASS_0', #\n",
    "        'SAVINGS_CAPITAL_CLASS_2',\n",
    "        'INVESTED_CAPITAL_CLASS_0',\n",
    "        'MIN_FEED_CLASS_0', # \n",
    "        'MIN_FEED_CLASS_1',\n",
    "        'REVENUES_CLASS_3', #\n",
    "        'PAYMENT_ACTIVITIES_CODE_0', \n",
    "        # 'CLIENTGROUP_0105', #\n",
    "        'CLIENTGROUP_0307',\n",
    "        'CLIENTGROUP_0101', #\n",
    "        'CROSS_SELL_SCORE',  # 'CROSS_SELL_SCORE_0',\n",
    "        'AGE_CLASS_Leeftijd_onbekend', \n",
    "        'HOMEBANK_COLOUR_unknown', \n",
    "        'HOMEBANK_COLOUR_Rood',\n",
    "        'LOYALITY_unknown', \n",
    "        'LOYALITY_Rood', \n",
    "    ]]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from seaborn import heatmap \n",
    "# plt.figure(figsize=(10, 7))\n",
    "# heatmap(df_cox.corr(), cmap='RdYlGn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cox.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lifelines import CoxPHFitter\n",
    "cph = CoxPHFitter()\n",
    "cph.fit(df_cox, duration_col='churn_time', event_col='event')\n",
    "cph.print_summary() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate, KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from src.pipe_store import sklearn_adapter\n",
    "random_state = 42\n",
    "\n",
    "X, y = sklearn_adapter(df_cox, label='event')\n",
    "# regs = [ LogisticRegression(), RandomForestClassifier()]\n",
    "X.drop('churn_time', axis=1, inplace=True)\n",
    "regs =  [ LogisticRegression(), RandomForestClassifier(), GaussianNB()]\n",
    "\n",
    "num_cols = ['CROSS_SELL_SCORE']\n",
    "column_transformer_scaler = ColumnTransformer([\n",
    "    ('Scaler', StandardScaler(), num_cols), \n",
    "], remainder='passthrough')\n",
    "\n",
    "results = {}\n",
    "for reg in regs:\n",
    "\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', column_transformer_scaler),\n",
    "        ('Model', reg),\n",
    "    ], verbose=False)\n",
    "\n",
    "    kfs = KFold(n_splits=5, shuffle=True)\n",
    "    # For the list of all metrics visit: https://scikit-learn.org/stable/modules/model_evaluation.html\n",
    "    metrics = ['recall', 'precision', 'roc_auc', 'accuracy', 'f1'] \n",
    "    scores = cross_validate(pipeline, X, y, cv=kfs, scoring=metrics)\n",
    "    # We will not use cross_val_score as it can only accept one metric\n",
    "    # print(scores)\n",
    "    reg_name = type(reg).__name__\n",
    "    results[reg_name] = {key: round(np.mean(val), 3) for key, val in scores.items()}\n",
    "pd.DataFrame(results).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X, y = sklearn_adapter(df_cox, label='event')\n",
    "X.drop('churn_time', axis=1, inplace=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, shuffle=True)\n",
    "clf = GaussianNB() # LogisticRegression() #GaussianNB()\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', column_transformer_scaler),\n",
    "    ('Model', clf),\n",
    "], verbose=False)\n",
    "\n",
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model_evaluation import (\n",
    "    plot_roc_curve,\n",
    "    plot_confusion_matrix,\n",
    "    plot_precision_recall_curve,\n",
    "    print_scores,\n",
    ")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X, y = sklearn_adapter(df_cox, label='event')\n",
    "X.drop('churn_time', axis=1, inplace=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, shuffle=True)\n",
    "clf = GaussianNB() # LogisticRegression() #GaussianNB()\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', column_transformer_scaler),\n",
    "    ('Model', clf),\n",
    "], verbose=False)\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "plot_confusion_matrix(pipeline, X_train, y_train)\n",
    "fig, ax = plt.subplots(1,2, figsize=(11, 4))\n",
    "plot_precision_recall_curve(pipeline, X_train, y_train, ax[0])\n",
    "plot_roc_curve(pipeline, X_train, y_train, ax[1])\n",
    "print_scores(pipeline, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(pipeline, X_test, y_test)\n",
    "print_scores(pipeline, X_test, y_test)\n",
    "fig, ax = plt.subplots(1,2, figsize=(10, 4))\n",
    "plot_roc_curve(pipeline, X_test, y_test, ax[0])\n",
    "plot_precision_recall_curve(pipeline, X_test, y_test, ax[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from src.model_evaluation import plot_calibration\n",
    "# plot_calibration(pipeline, X_test, y_test, n_bins=5, strategy='uniform')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# forest_importances = pd.Series(rcf.feature_importances_, index=X.columns)\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "# forest_importances.plot.bar( ax=ax)\n",
    "# ax.set_title(\"Feature importances using MDI\")\n",
    "# ax.set_ylabel(\"Mean decrease in impurity\")\n",
    "# fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.6 ('ml-bnfBc1U0')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "59fb0072ca45ddf16b4382c85a05d230ce260441c2c9c82e08dc46b2cdcf4637"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
